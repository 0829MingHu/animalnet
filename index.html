<!DOCTYPE html>
<!-- saved from url=(0031)http://youcook2.eecs.umich.edu/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="icon" type="image/png" href="http://umich.edu/skins/um2013/media/images/touch-icon-iphone.png">
  <title>YouCook2: Large-scale Cooking Video Dataset for Procedure Understanding and Description Generation</title>
  <link rel="stylesheet" href="./style/style.css">

</head>

<!-- Top  -->
<body>
<div class="top">
  <table>
    <tbody><tr>
      <td class="top-left">
      <img src="./style/michigan-logo.png" style="height:50px;width:auto">
      <span class="top-title-text">YouCook2 Dataset</span>
      </td>
      <td class="top-right">
        <a href="./index.html" style="color:#FFCB05">Home</a>
        <a href="./explore.html" style="color:#FFCB05">Explore</a>
        <a href="./download.html" style="color:#FFCB05">Download</a>
        <a href="./leaderboard.html" style="color:#FFCB05">Leaderboard</a>
      </td>
    </tr>
  </tbody></table>
</div>

<div class="contents">
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody><tr><td width="100%" valign="middle">
    <h2>Overview</h2>
    <div class="pc">
      YouCook2 is one of the largest task-oriented, instructional video dataset in the vision community.  It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. <strong>The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below).</strong> The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world. <a href="#">Explore</a> the dataset or <a href="#" target="_blank">read</a> more details.<br>

      YouCook2 is currently suitable for video-language research, weakly-supervised activity and object recognition in video, common object and action discovery across videos and procedure learning.<br>

      <font color="red"><b>(New!)</b></font> We released the dense bounding box annotation for objects in the recipe text. You can read more <a href="#" target="_blank">here</a> or <a href="#">download</a>.
    </div>
    
	<iframe src="https://www.bilibili.com/video/BV1c54y1c7H4/?spm_id_from=333.1007.tianma.4-3-13.click" quality="high" width="100%" height="400" align="middle" allowScriptAccess="sameDomain" type="application/x-shockwave-flash"class="intro-video"></iframe>
    <img class="chart-photo" src="./style/yc2.png" alt="YouCook2 example">
  </td></tr></tbody></table>
       
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody><tr><td width="100%" valign="middle">
    <h2>Statistics</h2>
    <div class="pc">
      The total video time is 176 hours with an average length of 5.26 mins for each video. Each video captured is within 10 mins and is recorded by camera devices but not slideshows. All the videos and precomputed feature can be downloaded in the <a href="#">Download page</a>.<br>
      Each video contains some number of procedure steps to fulfill a recipe. All the procedure segments are temporal localized in the video with starting time and ending time. The distributions of 1) video duration, 2) number of recipe steps per video, 3) recipe segment duration and 4) number of words per sentence are shown below.<br>
    </div>
    <img class="chart-photo" src="./style/yc2-stats.png" alt="YouCook2 stats">
    <div class="pc">
      YouCook2 also provides the language description for each procedure step. The total vocabulary appeared in the recipe corpus is over 2600 and the top 100 frequent actions/objects are shown in the following <a href="#" target="_blank">keyword cloud</a>.
    </div>
    <img class="chart-photo" src="./style/yc2-ann-keywords.png" alt="YouCook2 Vocab Cloud" align="middle" style="width:80%;height:auto;">
  </td></tr></tbody></table>
       
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody><tr><td width="100%" valign="middle">
   <h2>People</h2>
   <div class="pc">
    <div class="photo-desc">
      <a target="_blank" href="#">
        <img src="./style/luowei_photo.jpg" alt="photo" height="200">
      </a>
      <div class="desc">
        <a target="_blank" href="#"><strong>Luowei Zhou</strong></a><br> Research Scientist<br> Google Brain
      </div>
    </div>
    <div class="photo-desc">
      <a target="_blank" href="#">
        <img src="./style/nathan_photo.jpg" alt="photo" height="200">
      </a>
      <div class="desc">
        <a target="_blank" href="#"><strong>Nathan Louis</strong></a><br> Ph.D. Candidate <br> Department of EECS <br> University of Michigan
      </div>
    </div>
    <div class="photo-desc">
      <a target="_blank" href="#">
        <img src="./style/chenliang_photo.jpg" alt="photo" height="200">
      </a>
      <div class="desc">
        <a target="_blank" href="#"><strong>Chenliang Xu</strong></a><br> Associate Professor <br> Department of CS <br> University of Rochester
      </div>
    </div>
    <div class="photo-desc">
      <a target="_blank" href="#">
        <img src="./style/jason_photo.jpg" alt="photo" height="200">
      </a>
      <div class="desc">
        <a target="_blank" href="#"><strong>Jason Corso</strong></a><br>Professor <br> Department of EECS <br> University of Michigan
      </div>
    </div>
   </div>
  </td></tr></tbody></table>
       
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody><tr><td width="100%" valign="middle">
    <h2>Acknowledgement</h2>
    <div class="pc">
      <ul class="pcul">
        <li>This work has been supported in part by Google, ARO W911NF-15-1-0354, <a href="#" target="_blank">NSF NRI 1522904</a>, <a href="#" target="_blank">NSF CNS 1628987</a>, <a href="#" target="_blank">NSF CNS 1463102</a> and <a href="#" target="_blank">NSF BIGDATA 1741472</a>. We thank Mingyang Zhou, Yichen Yao, Haonan Chen, Carolyn Busch and Dali Zhang for their efforts in constructing the dataset. Please <a href="#" target="_top">contact</a> us if you have any question.
      </li></ul>
    </div>
  </td></tr></tbody></table>
</div>

</body></html>